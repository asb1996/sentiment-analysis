{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f50ec852",
   "metadata": {},
   "source": [
    "Imported important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccdf0fd7-1500-40cf-a775-7720876ef951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/asbedi/Conda_Env/nlp2023v2_v1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "import tqdm\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fb2ea6",
   "metadata": {},
   "source": [
    "Updated dataset library to get imdb dataset since previous versions had issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "374722d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994c2c6",
   "metadata": {},
   "source": [
    "splitting the imported imdb dataset into train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dac13a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = datasets.load_dataset(\"imdb\", split=[\"train\", \"test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f08c51df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25f260d",
   "metadata": {},
   "source": [
    "Initializing the Tokenizer from the 'bert-base-uncased' model. Following lines of code I ran inbuilt libraries from tokenizer to convert one sample sentence into tokens and visiulized what one training sample looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "250c23bc-6c34-4611-ba80-d270d002378f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8160d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ac4174b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'i',\n",
       " 'rented',\n",
       " 'i',\n",
       " 'am',\n",
       " 'curious',\n",
       " '-',\n",
       " 'yellow',\n",
       " 'from',\n",
       " 'my',\n",
       " 'video',\n",
       " 'store',\n",
       " 'because',\n",
       " 'of',\n",
       " 'all',\n",
       " 'the',\n",
       " 'controversy',\n",
       " 'that',\n",
       " 'surrounded',\n",
       " 'it',\n",
       " 'when',\n",
       " 'it',\n",
       " 'was',\n",
       " 'first',\n",
       " 'released',\n",
       " 'in',\n",
       " '1967',\n",
       " '.',\n",
       " 'i',\n",
       " 'also',\n",
       " 'heard',\n",
       " 'that',\n",
       " 'at',\n",
       " 'first',\n",
       " 'it',\n",
       " 'was',\n",
       " 'seized',\n",
       " 'by',\n",
       " 'u',\n",
       " '.',\n",
       " 's',\n",
       " '.',\n",
       " 'customs',\n",
       " 'if',\n",
       " 'it',\n",
       " 'ever',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'enter',\n",
       " 'this',\n",
       " 'country',\n",
       " ',',\n",
       " 'therefore',\n",
       " 'being',\n",
       " 'a',\n",
       " 'fan',\n",
       " 'of',\n",
       " 'films',\n",
       " 'considered',\n",
       " '\"',\n",
       " 'controversial',\n",
       " '\"',\n",
       " 'i',\n",
       " 'really',\n",
       " 'had',\n",
       " 'to',\n",
       " 'see',\n",
       " 'this',\n",
       " 'for',\n",
       " 'myself',\n",
       " '.',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " 'the',\n",
       " 'plot',\n",
       " 'is',\n",
       " 'centered',\n",
       " 'around',\n",
       " 'a',\n",
       " 'young',\n",
       " 'swedish',\n",
       " 'drama',\n",
       " 'student',\n",
       " 'named',\n",
       " 'lena',\n",
       " 'who',\n",
       " 'wants',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'everything',\n",
       " 'she',\n",
       " 'can',\n",
       " 'about',\n",
       " 'life',\n",
       " '.',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'she',\n",
       " 'wants',\n",
       " 'to',\n",
       " 'focus',\n",
       " 'her',\n",
       " 'attention',\n",
       " '##s',\n",
       " 'to',\n",
       " 'making',\n",
       " 'some',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'documentary',\n",
       " 'on',\n",
       " 'what',\n",
       " 'the',\n",
       " 'average',\n",
       " 'sw',\n",
       " '##ede',\n",
       " 'thought',\n",
       " 'about',\n",
       " 'certain',\n",
       " 'political',\n",
       " 'issues',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'vietnam',\n",
       " 'war',\n",
       " 'and',\n",
       " 'race',\n",
       " 'issues',\n",
       " 'in',\n",
       " 'the',\n",
       " 'united',\n",
       " 'states',\n",
       " '.',\n",
       " 'in',\n",
       " 'between',\n",
       " 'asking',\n",
       " 'politicians',\n",
       " 'and',\n",
       " 'ordinary',\n",
       " 'den',\n",
       " '##ize',\n",
       " '##ns',\n",
       " 'of',\n",
       " 'stockholm',\n",
       " 'about',\n",
       " 'their',\n",
       " 'opinions',\n",
       " 'on',\n",
       " 'politics',\n",
       " ',',\n",
       " 'she',\n",
       " 'has',\n",
       " 'sex',\n",
       " 'with',\n",
       " 'her',\n",
       " 'drama',\n",
       " 'teacher',\n",
       " ',',\n",
       " 'classmates',\n",
       " ',',\n",
       " 'and',\n",
       " 'married',\n",
       " 'men',\n",
       " '.',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " 'what',\n",
       " 'kills',\n",
       " 'me',\n",
       " 'about',\n",
       " 'i',\n",
       " 'am',\n",
       " 'curious',\n",
       " '-',\n",
       " 'yellow',\n",
       " 'is',\n",
       " 'that',\n",
       " '40',\n",
       " 'years',\n",
       " 'ago',\n",
       " ',',\n",
       " 'this',\n",
       " 'was',\n",
       " 'considered',\n",
       " 'pornographic',\n",
       " '.',\n",
       " 'really',\n",
       " ',',\n",
       " 'the',\n",
       " 'sex',\n",
       " 'and',\n",
       " 'nu',\n",
       " '##dity',\n",
       " 'scenes',\n",
       " 'are',\n",
       " 'few',\n",
       " 'and',\n",
       " 'far',\n",
       " 'between',\n",
       " ',',\n",
       " 'even',\n",
       " 'then',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'not',\n",
       " 'shot',\n",
       " 'like',\n",
       " 'some',\n",
       " 'cheap',\n",
       " '##ly',\n",
       " 'made',\n",
       " 'porn',\n",
       " '##o',\n",
       " '.',\n",
       " 'while',\n",
       " 'my',\n",
       " 'country',\n",
       " '##men',\n",
       " 'mind',\n",
       " 'find',\n",
       " 'it',\n",
       " 'shocking',\n",
       " ',',\n",
       " 'in',\n",
       " 'reality',\n",
       " 'sex',\n",
       " 'and',\n",
       " 'nu',\n",
       " '##dity',\n",
       " 'are',\n",
       " 'a',\n",
       " 'major',\n",
       " 'staple',\n",
       " 'in',\n",
       " 'swedish',\n",
       " 'cinema',\n",
       " '.',\n",
       " 'even',\n",
       " 'ing',\n",
       " '##mar',\n",
       " 'bergman',\n",
       " ',',\n",
       " 'arguably',\n",
       " 'their',\n",
       " 'answer',\n",
       " 'to',\n",
       " 'good',\n",
       " 'old',\n",
       " 'boy',\n",
       " 'john',\n",
       " 'ford',\n",
       " ',',\n",
       " 'had',\n",
       " 'sex',\n",
       " 'scenes',\n",
       " 'in',\n",
       " 'his',\n",
       " 'films',\n",
       " '.',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " '<',\n",
       " 'br',\n",
       " '/',\n",
       " '>',\n",
       " 'i',\n",
       " 'do',\n",
       " 'com',\n",
       " '##men',\n",
       " '##d',\n",
       " 'the',\n",
       " 'filmmakers',\n",
       " 'for',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'any',\n",
       " 'sex',\n",
       " 'shown',\n",
       " 'in',\n",
       " 'the',\n",
       " 'film',\n",
       " 'is',\n",
       " 'shown',\n",
       " 'for',\n",
       " 'artistic',\n",
       " 'purposes',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'just',\n",
       " 'to',\n",
       " 'shock',\n",
       " 'people',\n",
       " 'and',\n",
       " 'make',\n",
       " 'money',\n",
       " 'to',\n",
       " 'be',\n",
       " 'shown',\n",
       " 'in',\n",
       " 'pornographic',\n",
       " 'theaters',\n",
       " 'in',\n",
       " 'america',\n",
       " '.',\n",
       " 'i',\n",
       " 'am',\n",
       " 'curious',\n",
       " '-',\n",
       " 'yellow',\n",
       " 'is',\n",
       " 'a',\n",
       " 'good',\n",
       " 'film',\n",
       " 'for',\n",
       " 'anyone',\n",
       " 'wanting',\n",
       " 'to',\n",
       " 'study',\n",
       " 'the',\n",
       " 'meat',\n",
       " 'and',\n",
       " 'potatoes',\n",
       " '(',\n",
       " 'no',\n",
       " 'pun',\n",
       " 'intended',\n",
       " ')',\n",
       " 'of',\n",
       " 'swedish',\n",
       " 'cinema',\n",
       " '.',\n",
       " 'but',\n",
       " 'really',\n",
       " ',',\n",
       " 'this',\n",
       " 'film',\n",
       " 'doesn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'have',\n",
       " 'much',\n",
       " 'of',\n",
       " 'a',\n",
       " 'plot',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer.encode(train_data[0]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49622dc9",
   "metadata": {},
   "source": [
    "tokenize_and_convert_to_tensor function is created to take input data and converts all values belonging to 'text' key into\n",
    "tonkenizer keeping certain parameters in mind. Uses max-length to restrict the length of tokenizer here it will work to maintain the tokenized sentence length to 256-2 because it appends the [CLS] and [SEP] tokens at beginning and end. The truncate helps shorten the snetence to max-length value and in case the sentence is short padding will append 0's to match it to the length of all samples (which is max-length). Finally, it returns the tokens in a form of tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c967dd69-f913-495c-974b-8cbcc74d1eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_convert_to_tensor(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    return { key: value for key, value in tokenized_inputs.items()\n",
    "      }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c63f168",
   "metadata": {},
   "source": [
    "Code below will run the tokenizer function defined above on training data and return tensor values. Further it will pass these tensors into data loader to create batches. \n",
    "\n",
    "Later I added a print statement to check the type of train_dataset due to issues I faced during training. More info on this during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c3b975d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_data.map(tokenize_and_convert_to_tensor, batched=True)\n",
    "print(type(train_dataset['input_ids']))\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6cce70",
   "metadata": {},
   "source": [
    "Here I am initializing the Bert Model to acquire the word embeddings of the tokens from the bert-base-uncased model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23fcd32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "bertmodel = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460cb63e",
   "metadata": {},
   "source": [
    "Below i am defining my model which uses pytorch nn module and its transformer libraries to create the encoder layers and takes the input of embedding_size(comes from the word_embeddings), the total number of attention heads within an encoder and total number of layers defines the number of transformers stacked together. It also defines a linear layer with output of 2 (this is because of the binary nature of the sentiment classifier as the output can only be either positive or negative). \n",
    "\n",
    "Below the model class I am initializing the model with embedding size as 768 since bert model above returns word embeddings in that size. This initilization also includes dropout hyperparameter value set to 0.5.\n",
    "Following this code I also have the gradient descent function and loss calculator functions defined using the in-built libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8bb3822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/user/asbedi/Conda_Env/nlp2023v2_v1/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embedding_size, attention_heads, layers, dropout):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=embedding_size, nhead=attention_heads),\n",
    "                                            num_layers=layers)\n",
    "        self.fc = nn.Linear(embedding_size, 2)\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.mean(dim=1)\n",
    "        return self.fc(x)\n",
    "        \n",
    "model = TransformerEncoder(embedding_size=768, attention_heads=8, layers=3, dropout=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55388ba",
   "metadata": {},
   "source": [
    "The two cells below right before I get into training loop were created in order to check the dimensions of the input_ids, attention_mask and labels from the tokenizer, tensor and dataloader steps since during training each batch contained a list of tensors each of length 256(max-length set during tokenizer step). However, after exhaustive research I found that this behaviour is incorrect as it should be a single tensor of 256 length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "59ccf002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'])\n",
      "\n",
      "Type of input_ids:  <class 'list'>\n",
      "Length of input_ids:  256\n",
      "type of one instance of input_ids:  <class 'torch.Tensor'>\n",
      "length of one instance of input_ids:  5\n",
      "\n",
      "Type of attention_mask:  <class 'list'>\n",
      "Length of attention_mask:  256\n",
      "type of one instance of attention_mask:  <class 'torch.Tensor'>\n",
      "length of one instance of attention_mask:  5\n",
      "\n",
      "Type of label:  <class 'torch.Tensor'>\n",
      "Length of label:  5\n",
      "Shape of label:  torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch.keys())\n",
    "    print()\n",
    "\n",
    "    print('Type of input_ids: ', type(batch['input_ids']))\n",
    "    print('Length of input_ids: ', len(batch['input_ids']))\n",
    "    print('type of one instance of input_ids: ', type(batch['input_ids'][0]))\n",
    "    print('length of one instance of input_ids: ', len(batch['input_ids'][0]))\n",
    "    print()\n",
    "\n",
    "    print('Type of attention_mask: ', type(batch['attention_mask']))\n",
    "    print('Length of attention_mask: ', len(batch['attention_mask']))\n",
    "    print('type of one instance of attention_mask: ', type(batch['attention_mask'][0]))\n",
    "    print('length of one instance of attention_mask: ', len(batch['attention_mask'][0]))\n",
    "\n",
    "    print()\n",
    "    print('Type of label: ', type(batch['label']))\n",
    "    print('Length of label: ', len(batch['label']))\n",
    "    print('Shape of label: ', batch['label'].shape)\n",
    "    \n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1503b1ea",
   "metadata": {},
   "source": [
    "One of the main issues I faced was during training each batch in batches is supposed to be a hold a tensor belonging to input_ids of length 256 which is passed into Bert Model to provide the word embeddings for. However, it holds a list of 256 tensors. I had to run torch.stack to combine all tensors to bypass this issue. Further, I am having issues with comparing the output from my model that takes in word embeddings from Bert Model of size 768. \n",
    "\n",
    "The training is now breaking at the point of calculating loss since tensor returned from my model is 256x2 whereas label is a tensor of 1x5. My intuition is 5 comes from batch_size in dataloader. However, the problem either lies in calculating the output size for each layer in the model or the tokenizer method since it is outputting a list of tensor for each batch insteas of a tensor itself.\n",
    "\n",
    "Unfortunately i am not able to finish training due to issues i am hitting and would need more time to figure out the soluion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7b5ffdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of input_ids now  <class 'torch.Tensor'>\n",
      "shape of input-ids now  torch.Size([256, 5])\n",
      "\n",
      "type of word_embeddings <class 'torch.Tensor'>\n",
      "shape of word_embeddings  torch.Size([256, 5, 768])\n",
      "\n",
      "type of outputs <class 'torch.Tensor'>\n",
      "shape of outputs torch.Size([256, 2])\n",
      "tensor([[-7.5746e-01, -8.2665e-01],\n",
      "        [-5.4519e-01, -9.6301e-01],\n",
      "        [-6.5882e-01, -5.2784e-01],\n",
      "        [-8.6217e-01, -5.8591e-01],\n",
      "        [-1.7153e-02, -5.9775e-01],\n",
      "        [-2.7030e-01, -7.1750e-01],\n",
      "        [-3.2290e-01, -5.3839e-01],\n",
      "        [-7.6048e-01, -4.7219e-01],\n",
      "        [-4.8242e-01, -7.8979e-01],\n",
      "        [-5.8220e-01, -5.4729e-01],\n",
      "        [-8.0593e-01, -8.1654e-01],\n",
      "        [-4.0697e-01, -3.7847e-01],\n",
      "        [-7.8396e-01, -7.6987e-01],\n",
      "        [-6.3885e-01, -2.1720e-01],\n",
      "        [-1.2698e-01, -7.7166e-01],\n",
      "        [-8.6424e-01, -6.0170e-01],\n",
      "        [-4.6396e-01, -6.9555e-01],\n",
      "        [-3.9011e-01, -1.2904e+00],\n",
      "        [-4.6060e-01, -3.0133e-01],\n",
      "        [-6.1745e-01, -9.1420e-01],\n",
      "        [-4.0692e-01, -4.7873e-01],\n",
      "        [-7.4423e-01, -8.0914e-01],\n",
      "        [-3.5425e-01, -8.1397e-01],\n",
      "        [-4.2267e-01, -3.3958e-01],\n",
      "        [-5.4896e-01, -6.7728e-01],\n",
      "        [-6.5473e-01, -5.9270e-01],\n",
      "        [-5.5930e-02, -5.5729e-01],\n",
      "        [-5.9534e-01, -4.9625e-01],\n",
      "        [-5.7302e-01, -1.4552e-02],\n",
      "        [-8.5701e-01, -8.5601e-01],\n",
      "        [-3.9365e-01, -3.6570e-01],\n",
      "        [-3.3033e-01, -4.7590e-01],\n",
      "        [-5.1941e-01, -6.0316e-01],\n",
      "        [-2.6995e-01, -7.8490e-01],\n",
      "        [-4.2041e-01, -6.9530e-02],\n",
      "        [-6.6349e-01, -4.8672e-01],\n",
      "        [-4.3177e-01, -5.4506e-01],\n",
      "        [-5.6087e-01, -7.0926e-01],\n",
      "        [-6.4385e-01, -1.1246e+00],\n",
      "        [-4.2765e-01, -7.5449e-01],\n",
      "        [-5.0043e-01, -7.0151e-01],\n",
      "        [-3.2562e-01, -2.3140e-01],\n",
      "        [-6.6620e-01, -8.1132e-01],\n",
      "        [-3.5821e-01, -6.1740e-01],\n",
      "        [-6.9717e-01, -7.0301e-01],\n",
      "        [-1.8132e-01, -6.0849e-01],\n",
      "        [-5.3467e-01, -7.9267e-01],\n",
      "        [-2.4259e-01, -5.9502e-01],\n",
      "        [-3.8604e-01, -5.2822e-01],\n",
      "        [-4.0799e-01, -1.0684e+00],\n",
      "        [-4.8524e-01, -6.8642e-01],\n",
      "        [-5.6768e-02, -7.8400e-01],\n",
      "        [-4.6659e-01, -3.8146e-01],\n",
      "        [-4.2007e-01, -3.1207e-01],\n",
      "        [-7.4493e-01, -6.9467e-01],\n",
      "        [-7.4916e-01, -4.6052e-01],\n",
      "        [-7.5089e-01, -4.4321e-01],\n",
      "        [-4.1567e-01, -7.3358e-01],\n",
      "        [-4.8350e-01, -1.2232e-01],\n",
      "        [-6.0934e-01, -6.5731e-01],\n",
      "        [-8.9297e-01, -7.0821e-01],\n",
      "        [-5.6374e-01, -7.0986e-01],\n",
      "        [-6.1557e-01, -4.9602e-01],\n",
      "        [-3.5148e-01, -7.1115e-01],\n",
      "        [-4.6869e-01, -2.7370e-01],\n",
      "        [ 3.3733e-02, -6.6775e-02],\n",
      "        [-6.3264e-01, -2.4271e-01],\n",
      "        [-2.9253e-01, -4.7859e-01],\n",
      "        [-5.1748e-01, -7.4776e-01],\n",
      "        [-5.4211e-01, -5.8867e-01],\n",
      "        [-3.8449e-01, -7.3262e-01],\n",
      "        [-7.0700e-02, -5.2007e-01],\n",
      "        [-6.4983e-01, -3.5411e-01],\n",
      "        [-3.3277e-01, -9.5772e-01],\n",
      "        [-5.6143e-01, -1.1756e+00],\n",
      "        [-1.7339e-01, -1.0645e+00],\n",
      "        [-7.6348e-01, -6.1553e-01],\n",
      "        [-1.8791e-01, -3.3295e-01],\n",
      "        [-2.0074e-01, -5.4127e-01],\n",
      "        [-5.5394e-01, -3.6508e-01],\n",
      "        [-1.0894e+00, -1.7129e-01],\n",
      "        [-3.8516e-01, -5.5303e-01],\n",
      "        [-4.4536e-01, -7.3742e-01],\n",
      "        [-2.9126e-01, -8.9616e-01],\n",
      "        [ 4.3454e-02, -3.4152e-01],\n",
      "        [-2.3062e-01, -7.4978e-01],\n",
      "        [-5.8783e-01, -1.0430e+00],\n",
      "        [-6.1432e-01, -5.0747e-01],\n",
      "        [-2.1120e-03, -5.5461e-01],\n",
      "        [-8.6840e-01, -6.6016e-01],\n",
      "        [ 5.2102e-03, -7.1973e-01],\n",
      "        [-7.5531e-01, -4.5049e-01],\n",
      "        [-5.4787e-01, -1.0102e+00],\n",
      "        [-3.0499e-01, -6.5648e-01],\n",
      "        [-4.1614e-01, -5.3073e-01],\n",
      "        [-5.0362e-01, -7.4560e-01],\n",
      "        [-2.3532e-01, -1.2729e+00],\n",
      "        [-5.2632e-01, -6.6933e-01],\n",
      "        [-8.4665e-01, -6.2094e-01],\n",
      "        [-1.1558e-01, -9.2278e-01],\n",
      "        [-6.7872e-01, -7.1568e-02],\n",
      "        [-9.0875e-01, -8.8390e-01],\n",
      "        [-5.5068e-01, -4.8108e-01],\n",
      "        [-3.2514e-01, -6.7501e-01],\n",
      "        [-9.5127e-01, -6.0894e-01],\n",
      "        [-5.1895e-01, -1.2388e+00],\n",
      "        [-3.0372e-01, -6.6779e-01],\n",
      "        [-8.3551e-01, -6.2168e-01],\n",
      "        [-2.0829e-01, -2.8897e-01],\n",
      "        [-3.4174e-02, -2.8992e-01],\n",
      "        [-4.5680e-01, -4.3287e-01],\n",
      "        [-5.3605e-01, -9.2180e-01],\n",
      "        [-2.4644e-01, -8.9039e-01],\n",
      "        [-3.7385e-01, -7.5391e-01],\n",
      "        [-1.8475e-01, -1.1453e+00],\n",
      "        [-6.1159e-01, -8.0710e-01],\n",
      "        [-5.8227e-01, -5.8321e-01],\n",
      "        [-4.1810e-01, -8.0070e-01],\n",
      "        [-8.1984e-01, -5.9622e-01],\n",
      "        [-5.9479e-01, -5.3224e-01],\n",
      "        [-7.9539e-01, -6.6829e-01],\n",
      "        [-8.8983e-02, -7.9827e-01],\n",
      "        [-1.5759e-01, -6.2512e-01],\n",
      "        [-4.7128e-01, -6.6813e-01],\n",
      "        [-7.2139e-01, -1.0309e+00],\n",
      "        [-1.0553e+00, -9.6906e-01],\n",
      "        [ 1.0014e-01, -6.7820e-01],\n",
      "        [-5.1862e-01, -7.2950e-01],\n",
      "        [-7.8158e-01, -6.6443e-01],\n",
      "        [-2.0796e-01, -7.3320e-01],\n",
      "        [-1.8211e-01, -5.3055e-01],\n",
      "        [-6.5417e-01, -6.3160e-01],\n",
      "        [-5.2893e-01, -1.1674e+00],\n",
      "        [-1.0590e+00, -8.7045e-01],\n",
      "        [-2.1538e-04, -6.5568e-01],\n",
      "        [-7.0230e-01, -5.3534e-01],\n",
      "        [-5.3636e-01, -2.7466e-01],\n",
      "        [-4.2329e-01, -1.0354e+00],\n",
      "        [-3.0802e-01, -8.7237e-01],\n",
      "        [-4.4245e-01, -8.8991e-01],\n",
      "        [-3.3522e-01, -1.1213e+00],\n",
      "        [-5.2652e-01, -7.6559e-01],\n",
      "        [-5.0894e-01, -8.2085e-01],\n",
      "        [-3.7696e-01, -9.8092e-01],\n",
      "        [-9.3612e-01, -6.8017e-01],\n",
      "        [-6.1444e-01, -4.9887e-01],\n",
      "        [ 3.3517e-02, -4.0746e-01],\n",
      "        [-4.6784e-01, -6.9040e-01],\n",
      "        [-8.6057e-02, -1.0215e+00],\n",
      "        [-9.5196e-02, -1.0768e+00],\n",
      "        [-6.3590e-01, -3.5819e-01],\n",
      "        [-6.2084e-01, -4.0743e-01],\n",
      "        [-4.7043e-01, -9.0883e-01],\n",
      "        [-2.3055e-01, -6.1261e-01],\n",
      "        [-4.3203e-01, -7.2658e-01],\n",
      "        [-3.8194e-01, -1.0697e+00],\n",
      "        [-8.2787e-01, -9.4040e-01],\n",
      "        [ 1.1163e-01, -8.6280e-02],\n",
      "        [-3.9715e-01, -6.3114e-01],\n",
      "        [-8.1234e-01, -4.4766e-01],\n",
      "        [-6.5972e-01, -5.0186e-01],\n",
      "        [-6.4854e-01, -6.0607e-01],\n",
      "        [-3.5663e-01, -4.6841e-01],\n",
      "        [-4.9438e-01, -6.4639e-01],\n",
      "        [-4.4684e-01, -7.6534e-01],\n",
      "        [-6.5114e-01, -5.8690e-01],\n",
      "        [-8.4314e-01, -8.3572e-01],\n",
      "        [-8.5443e-01, -8.7664e-01],\n",
      "        [-5.8832e-01, -1.0108e+00],\n",
      "        [-9.2414e-01, -8.9590e-01],\n",
      "        [-2.3258e-01, -3.3694e-01],\n",
      "        [ 2.6292e-01, -5.1620e-01],\n",
      "        [-1.6990e-01, -4.7464e-01],\n",
      "        [-5.3647e-01, -7.5742e-01],\n",
      "        [-5.0436e-01, -8.1101e-01],\n",
      "        [-4.5825e-01, -9.0149e-01],\n",
      "        [-3.5931e-01, -8.3043e-01],\n",
      "        [-7.7622e-01, -1.1250e+00],\n",
      "        [-7.8946e-01, -6.0415e-01],\n",
      "        [-2.0899e-01, -1.3160e+00],\n",
      "        [-3.4743e-01, -6.1201e-01],\n",
      "        [-2.8081e-02,  7.3659e-02],\n",
      "        [-1.5636e-01, -6.0647e-01],\n",
      "        [-6.8432e-01, -5.8237e-01],\n",
      "        [-6.5095e-01, -1.0763e+00],\n",
      "        [-4.9149e-01, -3.6344e-01],\n",
      "        [-5.2726e-01, -8.3818e-01],\n",
      "        [-1.3214e-01, -1.2370e+00],\n",
      "        [-7.5317e-01, -8.5929e-01],\n",
      "        [-4.2108e-01, -6.5367e-01],\n",
      "        [-1.6593e-01, -4.3412e-01],\n",
      "        [-2.2371e-01, -8.4383e-01],\n",
      "        [-2.5237e-01, -7.0056e-01],\n",
      "        [-7.2518e-01, -8.5251e-01],\n",
      "        [-6.1754e-01, -4.6452e-01],\n",
      "        [-3.8372e-01, -9.6660e-01],\n",
      "        [-4.2776e-01, -7.7817e-01],\n",
      "        [-4.6840e-01, -9.0534e-01],\n",
      "        [-2.0095e-01, -4.3466e-01],\n",
      "        [-5.6790e-02, -5.6689e-01],\n",
      "        [-7.7109e-01, -8.3879e-01],\n",
      "        [-7.8111e-01, -7.0412e-01],\n",
      "        [-4.9068e-01, -5.6037e-01],\n",
      "        [-7.5752e-01, -4.5895e-01],\n",
      "        [-4.3766e-01, -5.3223e-01],\n",
      "        [-2.8090e-01, -5.3150e-01],\n",
      "        [-4.5859e-01, -1.7741e-01],\n",
      "        [-4.3616e-01, -7.3127e-01],\n",
      "        [-6.7889e-01, -4.9467e-01],\n",
      "        [-4.0065e-01, -8.3368e-01],\n",
      "        [-2.0665e-01, -7.2414e-01],\n",
      "        [-6.6531e-01, -9.5319e-01],\n",
      "        [-5.0970e-01, -4.6352e-01],\n",
      "        [ 2.0216e-02, -6.1194e-01],\n",
      "        [-5.3054e-01, -6.1106e-01],\n",
      "        [-7.7820e-01, -5.9759e-01],\n",
      "        [-1.8443e-01, -4.5503e-01],\n",
      "        [-7.0767e-02, -6.1509e-01],\n",
      "        [ 9.4825e-02, -2.9773e-01],\n",
      "        [-5.4802e-01, -5.8164e-01],\n",
      "        [-7.6864e-01, -8.0274e-01],\n",
      "        [-2.8949e-01, -7.3709e-01],\n",
      "        [-7.5413e-01, -8.0073e-01],\n",
      "        [-3.1053e-01, -6.4353e-01],\n",
      "        [-3.8076e-01, -2.9090e-01],\n",
      "        [-3.8807e-01, -4.0522e-01],\n",
      "        [-2.7393e-01, -5.8582e-01],\n",
      "        [-5.4247e-01, -4.7387e-01],\n",
      "        [-2.0831e-01, -6.8232e-01],\n",
      "        [-4.9719e-01, -7.9592e-01],\n",
      "        [-5.2275e-01, -5.7896e-01],\n",
      "        [-3.1856e-01, -9.4360e-01],\n",
      "        [-4.4486e-01, -6.4651e-01],\n",
      "        [-3.9726e-01, -5.6543e-01],\n",
      "        [-1.3558e-01, -8.7460e-01],\n",
      "        [-3.7529e-01, -2.1388e-01],\n",
      "        [-2.9182e-01, -2.5859e-01],\n",
      "        [-4.3854e-01, -1.0877e+00],\n",
      "        [-6.1957e-01, -6.5875e-01],\n",
      "        [-4.5135e-01, -1.1240e+00],\n",
      "        [-7.1107e-01, -2.3135e-01],\n",
      "        [-2.8597e-01, -2.3834e-01],\n",
      "        [-4.1901e-01, -7.9043e-01],\n",
      "        [-7.6672e-01, -5.4502e-01],\n",
      "        [-2.9024e-01, -4.5394e-01],\n",
      "        [-3.0846e-01, -6.4655e-01],\n",
      "        [-4.9794e-01, -5.5042e-01],\n",
      "        [-6.1639e-01, -7.5631e-01],\n",
      "        [ 1.4994e-01, -8.5502e-01],\n",
      "        [-6.5968e-01, -2.6082e-01],\n",
      "        [-3.2147e-01, -6.5804e-01],\n",
      "        [-5.6066e-01, -7.9416e-01],\n",
      "        [-5.2168e-01, -6.4767e-01],\n",
      "        [-5.7307e-01, -7.8801e-01],\n",
      "        [-5.8698e-01, -3.9217e-01],\n",
      "        [-4.0913e-02, -4.8209e-01]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch (got input: [256], target: [5])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 36\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m###\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Compare outputs with labels using CrossEntropyLoss\u001b[39;00m\n\u001b[1;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \n\u001b[1;32m     39\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \n",
      "File \u001b[0;32m/data/user/asbedi/Conda_Env/nlp2023v2_v1/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/user/asbedi/Conda_Env/nlp2023v2_v1/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data/user/asbedi/Conda_Env/nlp2023v2_v1/lib/python3.10/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/user/asbedi/Conda_Env/nlp2023v2_v1/lib/python3.10/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch (got input: [256], target: [5])"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    for batch in train_dataloader:\n",
    "        \n",
    "        batch['input_ids'] = torch.stack(batch['input_ids'])   \n",
    "        batch['attention_mask'] = torch.stack(batch['attention_mask'])   \n",
    "        \n",
    "        \n",
    "        #for debugging        \n",
    "        print('type of input_ids now ', type(batch['input_ids']))\n",
    "        print('shape of input-ids now ', batch['input_ids'].shape)\n",
    "        print()\n",
    "        ###\n",
    "        \n",
    "        outputembed = bertmodel(\n",
    "            input_ids=batch['input_ids'], \n",
    "            attention_mask=batch['attention_mask'])\n",
    "        \n",
    "        #embedding size is the 768 because of BERT MODEL\n",
    "        word_embeddings = (outputembed.last_hidden_state)\n",
    "        \n",
    "        #for debugging        \n",
    "        print('type of word_embeddings', type(word_embeddings))\n",
    "        print('shape of word_embeddings ', word_embeddings.shape)\n",
    "        print()\n",
    "        ###\n",
    "        \n",
    "        outputs = model(word_embeddings)\n",
    "        \n",
    "        #for debugging\n",
    "        print('type of outputs', type(outputs))\n",
    "        print('shape of outputs', outputs.shape)\n",
    "        print(outputs)\n",
    "        ###\n",
    "        \n",
    "        \n",
    "        loss = criterion(outputs[:, -1], batch['label']) \n",
    "\n",
    "        optimizer.zero_grad()  \n",
    "        loss.backward()  \n",
    "        optimizer.step() \n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45066e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c260f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp2023v2_v1]",
   "language": "python",
   "name": "conda-env-nlp2023v2_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
